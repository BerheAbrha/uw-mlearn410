Applied Machine Learning 410
========================================================
css: ../../assets/style/uw.css
author: Justin Donaldson
date: `r format(Sys.Date(), format="%B-%d-%Y")`
autosize: true

Clustering
---------------------------------
(AKA: Birds of a feather)
```{r setup}
require(GGally)
require(RColorBrewer)
opts_chunk$set(out.width='900px', dpi=200,cache=TRUE, fig.width=9, fig.height=5 )
local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org" 
       options(repos=r)
})
```


Clustering History
========================================================
Clustering first originated in *anthropology*
![kroeber](img/kroeber.jpg)
- Divide people into culturally similar groups
- "...matrilinear descent and avoidance of
relatives-in-law were or tended to be inherently connected."

***
![quant_expression](img/quant_expression.png)


Clustering Needs
========================================================
![clustering needs](img/cluster_desire.png)
“Data Clustering: 50 Years Beyond K-Means”, A.K. Jain (2008

Connectivity-based clustering
========================================================
type : sub-section

Hierarchical Clustering
============
Hierarchical clustering seeks to group together observations based on *proximity* in space.
For this analysis, let's look at arrest rates at a state level.
```{r}
  head(USArrests)
```

Hierarchical Clustering
============
The arrest information is not expressed in distances, so we need to transform it.
```{r}
  head(as.matrix(dist(USArrests)))
```

Hierarchical Clustering
============
R provides a good hierarchical clustering method in the standard distribution.
```{r}
  hc <-hclust(dist(USArrests))
  hc
```

Hierarchical Clustering
============
Plotting it gives us a representation of the clusters in a tree-like form.
```{r}
plot(hc, hang=-1)
```

Hierarchical Clustering
============
We can set a split point using "cutree", which will give us the desired number of clusters
```{r}
clusters = cutree(hc, k=5) # cut into 5 clusters
clusters
```

Hierarchical Clustering
============
We can overlay the cluster boundaries on the original plot.
```{r}
plot(hc)
rect.hclust(hc, k=5, border="purple")
```

Hierarchical Clustering
===========
```{r}
USArrests[clusters==1,]
USArrests[clusters==2,]
```

Hierarchical Clustering
===========
```{r,echo=FALSE}
arrests = USArrests
colors =brewer.pal(5, "Spectral")
arrests$clusters = colors[as.numeric(clusters)]
ggpairs(arrests, mapping=ggplot2::aes(fill=clusters,color=clusters))
```


Hierarchical Clustering
============
```{r}
head(as.matrix(UScitiesD))
```

Hierarchical Clustering
============
```{r}
hc = hclust(dist(UScitiesD))
plot(hc)
rect.hclust(hc, k=3, border="purple")
```

Hierarchical Clustering
============
Options - Distance (**good defaults**)
-----
- **Euclidean** $\|a-b \|_2 = \sqrt{\sum_i (a_i-b_i)^2}$
- Squared Euclidean $\|a-b \|_2^2 = \sum_i (a_i-b_i)^2$
- Manhattan $\|a-b \|_1 = \sum_i |a_i-b_i|$
- Maximum Distance $\|a-b \|_\infty = \max_i |a_i-b_i|$
- Mahalanobis Distance : ($S$  is the covariance matrix)* $\sqrt{(a-b)^{\top}S^{-1}(a-b)}$  

Hierarchical Clustering
============
Options - Linkage (**good defaults**)
-----
- single = closest neighbor to any point in cluster gets merged 
- **complete** = closest neighbor to furthest point in cluster gets added 
- UPGMA - Average of distances $d(x,y)$ : ${1 \over {|\mathcal{A}|\cdot|\mathcal{B}|}}\sum_{x \in \mathcal{A}}\sum_{ y \in \mathcal{B}} d(x,y)$
- centroid - distance between average of points in cluster.

Centroid-based clustering
========================================================
type : sub-section


K-Means
============
K-means is the classic centroid-based technique.
```{r}
  kc <-kmeans(dist(USArrests), 5)
  kc
```
K-Means
============
K-means is the classic centroid-based technique.
- Assignment Step : $S_i^{(t)} = \big \{ x_p : \big \| x_p - m^{(t)}_i \big \|^2 \le \big \| x_p - m^{(t)}_j \big \|^2 \ \forall j, 1 \le j \le k \big\}$
  - Assign each data point to cluster whose mean is the smallest within-cluster sum of squares.
- Update Step : $m^{(t+1)}_i = \frac{1}{|S^{(t)}_i|} \sum_{x_j \in S^{(t)}_i} x_j$
  - Calculate new centroids

K-Means
============
```{r}
  colors =brewer.pal(5, "Spectral")
  arrests$clusters = colors[kc$cluster]
  ggpairs(arrests, mapping=ggplot2::aes(fill=clusters,color=clusters))
```

K-Means
============
K-means is more sensitive to scaling
```{r}
  arrests = data.frame(scale(USArrests))
```

```{r, echo=FALSE}
  arrests = data.frame(scale(USArrests))
  kc <-kmeans(arrests, 5)
  arrests$clusters = colors[kc$cluster]
  ggpairs(arrests, mapping=ggplot2::aes(fill=clusters,color=clusters))
```

Quick aside on scaling
=====
Field measurements can have different *scales*
```{r}
dat = cbind(
  x=rnorm(100, mean =  0, sd = 1),
  y=rnorm(100, mean = -4, sd = 2),
  z=rnorm(100, mean =  4, sd = 9)
  )
boxplot(dat)
```

Quick aside on scaling
=====
In some cases it may be useful to *noramlize* the scales.
Standard normalization:
- mean = 0
- standard deviation = 1
```{r}
boxplot(scale(dat))
```

K-means compared to Hierarchical Clustering
======
Hierarchical Clustering
-----
- cluster count can be determined post hoc
- linkages are robust against scaling differences in fields
- good at extracting "strand" clusters
- quadratic time complexity : ~ $O(n^2)$
- repeatable

K-Means
----
- "k" cluster count must be given up front.
- scaling affects centroid calculation, consider normalizing if possible.
- good at extracting "spherical" clusters
- near-linear time complexity : ~ $O(n)$
- requires random initialization, results may not be repeatable in some cases

Distribution-based clustering
========================================================
type : sub-section

Expectation Maximization
==========
```{r}
require(EMCluster)
```
- based on probability distributions
- no hard assignment of points to clusters, instead use probabilities
- underlying distributions can be anything, gaussians are common 
- optimize : $Q(\boldsymbol\theta|\boldsymbol\theta^{(t)}) = \operatorname{E}_{\mathbf{Z}|\mathbf{X},\boldsymbol\theta^{(t)}}\left[ \log L (\boldsymbol\theta;\mathbf{X},\mathbf{Z})  \right]$

Expectation Maximization
==========
EM works well when you have a good idea of the underlying distribution of the data.
In this case, normal deviates
```{r}
means = c(0,20,-15)
x = rnorm(300, means[1], 10)
y = rnorm(100, means[2], 1)
z = rnorm(100, means[3], 1)
dat = data.frame(x=c(x,y,z))
ggplot(data = dat,aes(x=x)) + geom_histogram()
```

Expectation Maximization
==========
EM fits a GMM to the data
```{r}
library(EMCluster)
em = shortemcluster(dat, simple.init(dat, nclass = 3))
em = emcluster(dat, em, assign.class=T)
colors = brewer.pal(3, "Spectral")[em$class]
dat$class = colors
ggplot(data = dat, aes(x=x, fill=class)) + geom_histogram(position="dodge")

```

Expectation Maximization
==========
EM models the distribution parameters, and can capture them even if distributions overlap.
```{r}
em$Mu
as.matrix(means)
```

Expectation Maximization
==========
Considerations
- If distribution characteristics are known, EM works well at recovering from noise and overlap in cluster boundaries.
- Parameterization includes cluster count *and* mixture model specification. 
- Most similar to K-means, except cluster assignment is *soft*, rather than *hard* as in km.
